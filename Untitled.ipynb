{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8a358-6713-4e24-9841-2094243d0b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Breit-Wigner model\n",
    "def breit_wigner(e, param_vec):\n",
    "    e_center, gamma, strength = param_vec\n",
    "    denom = ((e - e_center)**2) + ((gamma**2)/4)\n",
    "    return strength / denom\n",
    "\n",
    "# Chi-squared cost function\n",
    "def breit_wigner_chi(x_data, y_data, sigma_vals, param_vec):\n",
    "    model_vals = breit_wigner(x_data, param_vec)\n",
    "    residuals = (y_data - model_vals) / sigma_vals\n",
    "    return np.sum(residuals**2)\n",
    "\n",
    "# Wrapper for use in gradient descent (phi takes just xs)\n",
    "def make_phi(x_data, y_data, sigma_vals):\n",
    "    def phi(xs):\n",
    "        return breit_wigner_chi(x_data, y_data, sigma_vals, xs)\n",
    "    return phi\n",
    "\n",
    "# Numerical gradient (finite difference)\n",
    "def gradient(phi, xs, h=1.e-6):\n",
    "    n = xs.size\n",
    "    phi0 = phi(xs)\n",
    "    Xph = (xs*np.ones((n,n))).T + np.identity(n)*h\n",
    "    grad = (np.array([phi(xph) for xph in Xph]) - phi0)/h\n",
    "    return grad\n",
    "\n",
    "# Termination criterion\n",
    "def termcrit(old, new):\n",
    "    return np.linalg.norm(new - old)\n",
    "\n",
    "# Gradient descent algorithm\n",
    "def descent(phi, gradient, xolds, gamma=0.15, kmax=200, tol=1.e-8):\n",
    "    for k in range(1, kmax):\n",
    "        grad = gradient(phi, xolds)\n",
    "        xnews = xolds - gamma * grad\n",
    "        err = termcrit(xolds, xnews)\n",
    "        print(f\"{k:3d} {xnews} err={err:.2e} chi^2={phi(xnews):.6f}\")\n",
    "        if err < tol:\n",
    "            break\n",
    "        xolds = np.copy(xnews)\n",
    "    else:\n",
    "        xnews = None\n",
    "    return xnews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9dcd7f-2155-4c4c-8d62-f110e27c333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import pandas as p   d\n",
    "\n",
    "# Define the Rosenbrock function\n",
    "def rosenbrock(x, a=1, b=100):\n",
    "    return (a - x[0])**2 + b * (x[1] - x[0]**2)**2\n",
    "\n",
    "# Define the gradient of the Rosenbrock function\n",
    "def rosenbrock_grad(x, a=1, b=100):\n",
    "    dx = -2*(a - x[0]) - 4*b*x[0]*(x[1] - x[0]**2)\n",
    "    dy = 2*b*(x[1] - x[0]**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Define the Hessian of the Rosenbrock function (for trust-region methods)\n",
    "def rosenbrock_hess(x, a=1, b=100):\n",
    "    dxx = 2 - 4*b*x[1] + 12*b*x[0]**2\n",
    "    dxy = -4*b*x[0]\n",
    "    dyy = 2*b\n",
    "    return np.array([[dxx, dxy], [dxy, dyy]])\n",
    "\n",
    "# List of optimization methods\n",
    "methods = [\n",
    "    \"Nelder-Mead\", \"BFGS\", \"CG\", \"L-BFGS-B\", \"TNC\",\n",
    "    \"SLSQP\", \"Powell\", \"trust-constr\", \"dogleg\"\n",
    "]\n",
    "\n",
    "# Initial guess\n",
    "x0 = np.array([-1.2, 1.0])\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Run each optimizer\n",
    "for method in methods:\n",
    "    options = {'disp': False, 'maxiter': 1000}\n",
    "    kwargs = {'method': method, 'options': options}\n",
    "    \n",
    "    # Add gradient and Hessian if applicable\n",
    "    if method in ['BFGS', 'CG', 'L-BFGS-B', 'TNC', 'SLSQP']:\n",
    "        kwargs['jac'] = rosenbrock_grad\n",
    "    if method in ['trust-constr', 'dogleg']:\n",
    "        kwargs['jac'] = rosenbrock_grad\n",
    "        kwargs['hess'] = rosenbrock_hess\n",
    "\n",
    "    res = minimize(rosenbrock, x0, **kwargs)\n",
    "    results.append({\n",
    "        'Method': method,\n",
    "        'Success': res.success,\n",
    "        'Final Value': res.fun,\n",
    "        'Solution': res.x,\n",
    "        'Iterations': res.nit if 'nit' in res else None,\n",
    "        'Function Evaluations': res.nfev if 'nfev' in res else None,\n",
    "        'Message': res.message\n",
    "    })\n",
    "\n",
    "# Print results as DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.sort_values(by=\"Final Value\").reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c1eb94-2a58-4e4f-9c96-64ae3eecd10d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     55\u001b[39m     xolds = np.array([\u001b[32m2.\u001b[39m, \u001b[32m0.25\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     xnews = descent(\u001b[43mphi\u001b[49m, gradient, xolds)\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mprint\u001b[39m(xnews)\n",
      "\u001b[31mNameError\u001b[39m: name 'phi' is not defined"
     ]
    }
   ],
   "source": [
    "# Implement gradient descent as listed in Ch. 5.6 of Gezerlis (see Code 5.7), \n",
    "# but the scalar function is the cost function (chi-squared) as described \n",
    "# above.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "en, f, f_sd = np.loadtxt(\"data_ch6_resonance.txt\")\n",
    "\n",
    "#descent.py\n",
    "from jacobi import termcrit\n",
    "import numpy as np\n",
    "\n",
    "# Breit-Wigner model\n",
    "def breit_wigner(e, param_vec):\n",
    "    e_center, gamma, strength = param_vec\n",
    "    denom = ((e - e_center)**2) + ((gamma**2)/4)\n",
    "    return strength / denom\n",
    "\n",
    "# Chi-squared cost function\n",
    "def breit_wigner_chi(x_data, y_data, sigma_vals, param_vec):\n",
    "    numer = y_data - breit_wigner(x_data, param_vec)\n",
    "    denom = sigma_vals\n",
    "    return np.sum((numer/denom)**2)\n",
    "\n",
    "#replace scalar function with cost function chi-squared\n",
    "def make_phi(x_data, y_data, sigma_vals):\n",
    "    def phi(xs):\n",
    "        return breit_wigner_chi(x_data, y_data, sigma_vals, xs)\n",
    "    return phi\n",
    "\n",
    "#gradient\n",
    "def gradient(phi,xs,h=1.e-6):\n",
    "    n = xs.size\n",
    "    phi0 = phi(xs)\n",
    "    Xph = (xs*np.ones((n,n))).T + np.identity(n)*h\n",
    "    grad = (phi(Xph) - phi0)/h\n",
    "    return grad\n",
    "\n",
    "#descent\n",
    "def descent(phi,gradient,xolds,gamma=0.15,kmax=200,tol=1.e-8):\n",
    "    for k in range(1,kmax):\n",
    "        xnews = xolds - gamma*gradient(phi,xolds)\n",
    "\n",
    "        err = termcrit(xolds,xnews)\n",
    "        if err < tol:\n",
    "            break\n",
    "\n",
    "        xolds = np.copy(xnews)\n",
    "    else:\n",
    "        xnews = None\n",
    "    return xnews\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    xolds = np.array([2., 0.25])\n",
    "    xnews = descent(phi, gradient, xolds)\n",
    "    print(xnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24f21c-5f02-4020-af17-e3f3ea33a9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
